---
title: Output Stream
mode: 'wide'
---

Passing `stream: true` to the `message.create` method will return an async iterator that yields response objects.

```typescript
const res = await lira.message.create({
  [...]
  stream: true,
  [...]
})

for await (const message of res) {
  console.log(message)
}
```

## Response object

ðŸ‘Œ No matter the `provider` used the response object will stay consistent to the following schema

```typescript
{
  id?: string
  model?: string
  message: AssistantResponse | ToolUseResponse
  stop_reason?: StopReason
  stop_sequence?: string
  usage?: Usage
  logprobs?: Logprobs
  openai_options?: OpenAIOptions
}
```

<b>[ðŸ‘‰ Response object](/sdk-reference/messages/output/static)</b>
<br />
The response object is the same as the one used in the static output. The only
difference is that the `id` and `model` fields are optional.
<br />
This is because the streaming response object is not the final response object.
It's a partial response object that will be updated as the model generates more
tokens. So the `id` and `model` fields might be generated at any step in the
stream.
