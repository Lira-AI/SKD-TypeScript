---
title: Output Static
mode: 'wide'
---

Passing `stream: false` to the `message.create` method will return a single response object. This is the default behavior.

```typescript
const res = await lira.message.create({
  [...]
  stream: false // Default value
  [...]
})
```

## Response object

ðŸ‘Œ No matter the `provider` used the response object will stay consistent to the following schema

```typescript
{
  id: string
  model: string
  message: AssistantResponse | ToolUseResponse
  stop_reason: StopReason
  stop_sequence?: string
  usage?: Usage
  logprobs?: Logprobs
  openai_options?: OpenAIOptions
}
```

<ResponseField name="id" type="string" required>
  The unique identifier of the response.
</ResponseField>

<ResponseField name="model" type="string" required>
  The model used to generate the response.
  <Note>
    Usually the model is the same as the one used in the request. But in some
    cases, the model name can be slightly different. It's best to prefer the
    input model name, which stays consistent.
  </Note>
</ResponseField>

<ResponseField
  name="message"
  type="AssistantResponse | ToolUseResponse"
  required
>
  The response message generated by the model.
  <Expandable title="assistant">
  The model response.
  `Text` based message:
  ```typescript
  {
    role: 'assistant',
    type: 'text'
    content: string
  }
  ```
  </Expandable>

  <Expandable title="Tool Use">
  The model response to use a tool.
  ```typescript
  {
    role: "tool_use",
    tools: Array<{
      type: 'function'
      data: {
        arguments: string
        name: string
      }
    }>
  }
  ```
  </Expandable>
</ResponseField>

<ResponseField
  name="stop_reason"
  type="'max_tokens' | 'stop_sequence' | 'stop'"
  required
>
  The reason why the model stopped generating tokens.
  <ul>
    <li>
      <code>max_tokens</code>: The model reached the maximum token limit.
    </li>
    <li>
      <code>stop_sequence</code>: The model incountered one of the provided
      <b>stop_sequence</b> tokens.
    </li>
    <li>
      <code>stop</code>: The normal stop condition, the model reached the end of
      the response.
    </li>
  </ul>
</ResponseField>

<ResponseField name="stop_sequence" type="string">
  The stop_sequence token that caused the model to stop generating tokens. In
  case the stop_reason is not <code>stop_sequence</code>, this field will be
  undefined.
</ResponseField>

<ResponseField
  name="usage"
  type="Usage"
>
  The usage object contains the number of tokens used in the input and output.
  ```typescript
  {
    input_tokens?: number
    output_tokens?: number
  }
  ```
</ResponseField>

<ResponseField name="logprobs" type="Logprobs">
  The logprobs object contains the log probabilities of the tokens generated by
  the model.
  ```typescript
  Array<{
    token: string
    logprob: number
    bytes?: Bytes
    top_logprobs?: Array<{ token: string; logprob: number; bytes?: Bytes }>
  }>
  ```
</ResponseField>

<ResponseField name="openai_options" type="OpenAIOptions">
  The openai_options object contains the options used by the OpenAI service.
  ```typescript
  {
    created: number
    service_tier?: 'scale' | 'default'
    system_fingerprint?: string
  }
  ```
  <b>NOTE:</b> This key will be undefined if the provider is not OpenAI.
</ResponseField>
